image_folder: "/export/home/werbya/VLA_Embodied_AI/data/clip3_frames"
start_frame: 1600
end_frame: 1700

vlm_model:
  name: "Salesforce/xgen-mm-phi3-mini-instruct-r-v1"
  cache_dir: "model_cache"
  queries:
    - "provide a detailed description of the scene"
    - "Describe what is loader doing loading or unloading material?"
    # - "is the loader interacting with a truck or not?"
    - "What is the status of the loader bucket?"
    - "Is the loader bucket up on the air or down on the ground?"
    - "Is there a pile of dirt in front of the loader? is the loader interacting with it?"

detector_model:
  name: "google/owlvit-base-patch32"
  cache_dir: "model_cache"
  confidence_threshold: 0.1
  target_classes: 
    - "pile of dirt"
    - "pile of rocks"
    - "pile of sand"
    - "Truck"
    - "sand"
    - "dirt"
    - "rocks"

llama:
  base_model_path: "meta-llama/Llama-3.2-1B-Instruct"
  new_model_path: "/export/home/werbya/VLA_Embodied_AI/llama-3.2-1b-sensmore-QA"


hydra:
  run:
    dir: '.'    
  sweep:
    dir: '.'